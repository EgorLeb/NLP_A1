{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are:\n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu0I6koItIm_"
      },
      "source": [
        "# Getting the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [],
      "source": [
        "freq = dict()\n",
        "words = set()\n",
        "with open(\"big.txt\", 'r') as f:\n",
        "    text = f.read().lower()\n",
        "    i = 0\n",
        "    word = []\n",
        "    while i < len(text):\n",
        "        if text[i].isalpha():\n",
        "            word.append(text[i])\n",
        "            i += 1\n",
        "        else:\n",
        "            if len(word) != 0:\n",
        "                words.add(''.join(word))\n",
        "                freq[''.join(word)] = freq.get(''.join(word), 0) + 1\n",
        "            i += 1\n",
        "            word = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cwrArW2uhou",
        "outputId": "bfa03188-d37d-42db-8c6c-2263e6080d62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29157"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "1UOisef2u0SO"
      },
      "outputs": [],
      "source": [
        "with open(\"fivegrams (2).txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        n, w1, w2, w3, w4, w5 = line.split()\n",
        "        words.add(w1)\n",
        "        freq[w1] = freq.get(w1, 0) + 1\n",
        "        words.add(w2)\n",
        "        freq[w2] = freq.get(w2, 0) + 1\n",
        "        words.add(w3)\n",
        "        freq[w3] = freq.get(w3, 0) + 1\n",
        "        words.add(w4)\n",
        "        freq[w4] = freq.get(w4, 0) + 1\n",
        "        words.add(w5)\n",
        "        freq[w5] = freq.get(w5, 0) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kv8je2xu3b0",
        "outputId": "f3c0d220-d854-4088-c2c9-5cee17cc0c82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40187"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay1V_1MIwBuW"
      },
      "source": [
        "# Getting all the typos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "RyzcCqmLxuHf"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "def typos(words):\n",
        "    res = {}\n",
        "    for word in words:\n",
        "        # delete\n",
        "        for i in range(len(word)):\n",
        "            new_word = word[:i] + word[i+1:]\n",
        "            if new_word not in res:\n",
        "                res[new_word] = set()\n",
        "            res[new_word].add(word)\n",
        "        # transpose\n",
        "        for i in range(len(word)-1):\n",
        "            new_word = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
        "            if new_word not in res:\n",
        "                res[new_word] = set()\n",
        "            res[new_word].add(word)\n",
        "        # replace\n",
        "        for i in range(len(word)):\n",
        "            for character in string.ascii_lowercase:\n",
        "                new_word = word[:i] + character + word[i+1:]\n",
        "                if new_word not in res:\n",
        "                    res[new_word] = set()\n",
        "                res[new_word].add(word)\n",
        "        # insert\n",
        "        for i in range(len(word) + 1):\n",
        "            for character in string.ascii_lowercase:\n",
        "                new_word = word[:i] + character + word[i:]\n",
        "                if new_word not in res:\n",
        "                    res[new_word] = set()\n",
        "                res[new_word].add(word)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwlkWLROqCxY"
      },
      "source": [
        "# get first itaration typos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "tmrOhZhzwBN3"
      },
      "outputs": [],
      "source": [
        "err2word1 = typos(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2r3eEmx0WX8"
      },
      "source": [
        "# BiGram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "LYC8Q8hq0ZSj"
      },
      "outputs": [],
      "source": [
        "bigrams = {}\n",
        "with open(\"bigrams (2).txt\", encoding='latin-1') as f:\n",
        "    cn = 0\n",
        "    for line in f:\n",
        "        n, w1, w2 = line.split()\n",
        "        if w1 not in bigrams:\n",
        "            bigrams[w1] = {}\n",
        "        bigrams[w1][w2] = int(n)\n",
        "        freq[w1] = freq.get(w1, 0) + int(n)\n",
        "        freq[w2] = freq.get(w2, 0) + int(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN4dBLCysepY"
      },
      "source": [
        "# Main algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "OWyDAtd9scCT"
      },
      "outputs": [],
      "source": [
        "def get_candidates(word):\n",
        "    if word in err2word1:\n",
        "        return err2word1[word]\n",
        "    err2 = typos(typos([word]).keys()).keys()\n",
        "    res = []\n",
        "    for error_word in err2:\n",
        "        if error_word in words:\n",
        "            res.append(error_word)\n",
        "    return set(res)\n",
        "\n",
        "\n",
        "def my_main(sentance: list[str]) -> list[str]:\n",
        "    for i in range(len(sentance)):\n",
        "        if sentance[i] not in words:\n",
        "            cands = list(get_candidates(sentance[i]))\n",
        "            if len(cands) == 0:\n",
        "                continue\n",
        "            bgr = [0] * len(cands)\n",
        "            bgl = [0] * len(cands)\n",
        "            for j in range(1, len(cands)):\n",
        "                if cands[j - 1] in bigrams and cands[j] in bigrams[cands[j - 1]]:\n",
        "                    bgr[j] = bigrams[cands[j - 1]][cands[j]]\n",
        "                    bgl[j-1] = bigrams[cands[j - 1]][cands[j]]\n",
        "            bg_final = [bgr[j] + bgl[j] + freq.get(cands[j], 0) / 100_000_000 for j in range(len(cands))]\n",
        "            sentance[i] = cands[bg_final.index(max(bg_final))]\n",
        "    return sentance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_main(\"dking sport\".split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv2HtasSnduk",
        "outputId": "2ec018d3-f52f-4459-d11f-e9e7397ab0f6"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doing', 'sport']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "#### 1\n",
        "\n",
        "I got all word which I can get and their frequency\n",
        "\n",
        "#### 2\n",
        "\n",
        "I create func `tions` and first iteration errors for each word\n",
        "\n",
        "#### 3\n",
        "\n",
        "I create bigram by file `bigrams (2).txt`\n",
        "\n",
        "#### 4\n",
        "\n",
        "I write `my_main` function that:\n",
        "\n",
        "* choosing candidates: consider candidates by priority - 0 errors, 1 errors, 2 errors (if errors more then 2, I don't change the word)\n",
        "\n",
        "* for each candidate (if more than 1) I consider next and previous word in birgham, and if I have the pair in birgham I add frequency in sum\n",
        "\n",
        "* if I have several candidates with equal sums, I compare it by their frequency of words (frequency / 100_000_000 -> (0 - 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset"
      ],
      "metadata": {
        "id": "Mk-oa8gD0kep"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "true_text = list()\n",
        "with open(\"True_spases.txt\", 'r') as f:\n",
        "    text = f.read().lower()\n",
        "    i = 0\n",
        "    word = []\n",
        "    while i < len(text):\n",
        "        if text[i].isalpha():\n",
        "            word.append(text[i])\n",
        "            i += 1\n",
        "        else:\n",
        "            if len(word) != 0:\n",
        "                true_text.append(''.join(word))\n",
        "            i += 1\n",
        "            word = []\n",
        "    if len(word) != 0:\n",
        "        true_text.append(''.join(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error_text = list()\n",
        "with open(\"error.txt\", 'r') as f:\n",
        "    text = f.read().lower()\n",
        "    i = 0\n",
        "    word = []\n",
        "    while i < len(text):\n",
        "        if text[i].isalpha():\n",
        "            word.append(text[i])\n",
        "            i += 1\n",
        "        else:\n",
        "            if len(word) != 0:\n",
        "                error_text.append(''.join(word))\n",
        "            i += 1\n",
        "            word = []\n",
        "    if len(word) != 0:\n",
        "        error_text.append(''.join(word))"
      ],
      "metadata": {
        "id": "QpddeN4OxLmk"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(error_text) == len(true_text)"
      ],
      "metadata": {
        "id": "9p-e506CxQbh"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Norvig's solution"
      ],
      "metadata": {
        "id": "5oLFEQb30pY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def get_words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(get_words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def norvigs_main(sentance: list[str]) -> list[str]:\n",
        "    return [correction(word) for word in sentance]"
      ],
      "metadata": {
        "id": "mibJnaL00YrU"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norvigs_main(\"dking sport\".split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Tx25wJ1ruB",
        "outputId": "b5245598-abf8-4e0b-bab0-c11d34c82fdd"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['king', 'sport']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare"
      ],
      "metadata": {
        "id": "zTChfyXz10eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_errors = sum([1 if error_text[i] != true_text[i] else 0 for i in range(len(true_text))])"
      ],
      "metadata": {
        "id": "JlhWRPC015TV"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_errors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKYMpiDs4QhR",
        "outputId": "d64e0b9f-1af0-46bd-a8db-e9bc6eaa002e"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20323"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norvigs_res = norvigs_main(error_text)"
      ],
      "metadata": {
        "id": "VINssW2w4Vu9"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norvigs_errors = sum([1 if norvigs_res[i] != true_text[i] else 0 for i in range(len(true_text))])"
      ],
      "metadata": {
        "id": "xAFrVK9F4dp5"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_res = my_main(error_text)"
      ],
      "metadata": {
        "id": "voIier5W4l_m"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_errors = sum([1 if my_res[i] != true_text[i] else 0 for i in range(len(true_text))])"
      ],
      "metadata": {
        "id": "EMsBG2bW4mSk"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"norvig's accuracy: {norvigs_errors/total_errors*100}%\")\n",
        "print(f\"my accuracy: {my_errors/total_errors*100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZk9EPIQ44KI",
        "outputId": "6fe3975f-b3a1-4068-b47e-cb52efb9c785"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norvig's accuracy: 36.864636126556114%\n",
            "my accuracy: 37.36160999852384%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I expected a bigger difference, but it looks like I have a bad implementation or the norvig's approach is difficult to modify much :)"
      ],
      "metadata": {
        "id": "ELbLEW38-Ety"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9KcxUNdgmlV"
      },
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}